{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac840170-72d3-419a-9a2e-6fe82ed1aab0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ddc7f96-486b-4d06-a970-05bb2a97c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "GRADIENT = os.path.exists('train')\n",
    "KAGGLE = os.path.exists('../input')\n",
    "\n",
    "if KAGGLE:\n",
    "    sys.path.insert(0, '../input/uwmadisonutils/')\n",
    "    data_path = '../input/uw-madison-gi-tract-image-segmentation/'\n",
    "    \n",
    "elif GRADIENT:\n",
    "    sys.path.insert(0, 'uwmadisonutils/')\n",
    "    data_path = ''\n",
    "\n",
    "from uwmadisonutils.utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14765130-29b9-4c39-ba10-a59b3c2b5687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if KAGGLE:\n",
    "    !cp -r ../input/pytorch-segmentation-models-lib/ ./\n",
    "    !pip install -q ./pytorch-segmentation-models-lib/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4\n",
    "    !pip install -q ./pytorch-segmentation-models-lib/efficientnet_pytorch-0.6.3/efficientnet_pytorch-0.6.3\n",
    "    !pip install -q ./pytorch-segmentation-models-lib/timm-0.4.12-py3-none-any.whl\n",
    "    !pip install -q ./pytorch-segmentation-models-lib/segmentation_models_pytorch-0.2.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f62756-7a35-4adb-ab07-b2ecd99c1524",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab6f62d-8c7a-4bb5-ab4e-531912f58457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = Path(data_path+'train')\n",
    "test_path = Path(data_path+'test')\n",
    "\n",
    "train = pd.read_csv(data_path+'train.csv', low_memory=False)\n",
    "sample_submission = pd.read_csv(data_path+'sample_submission.csv', low_memory=False).rename(columns={'predicted': 'segmentation'})\n",
    "\n",
    "train_path = Path(data_path+'train')\n",
    "test_path = Path(data_path+'test')\n",
    "\n",
    "train_fnames = get_image_files(train_path)\n",
    "test_fnames = get_image_files(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c00e8-c7ef-4074-a424-ead264718d95",
   "metadata": {},
   "source": [
    "## Train and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1868934-3f46-438f-a311-ed3868baf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'exp-35'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a504591b-055f-4fbb-9f4a-9eced2281ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "stride = 2\n",
    "\n",
    "train = pd.read_csv(data_path+'train.csv', low_memory=False)\n",
    "train = get_custom_df(train, train_fnames, str(train_path), channels=channels, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c740334f-41f2-42b4-a825-b283aa366b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_aug(img_size, crop=0.9, resize=0.5, p=0.4):\n",
    "    crop_size = round(img_size[0]*crop)\n",
    "    resize = round(crop_size*resize)\n",
    "    return albumentations.Compose([\n",
    "                albumentations.RandomCrop(height=crop_size, width=crop_size, always_apply=True),\n",
    "                albumentations.HorizontalFlip(),\n",
    "                albumentations.OneOf([\n",
    "                    albumentations.Sharpen(),\n",
    "                    albumentations.Emboss(),\n",
    "                    albumentations.RandomGamma(),\n",
    "                    albumentations.RandomBrightnessContrast(),\n",
    "                    ], p=p),\n",
    "                albumentations.OneOf([\n",
    "                    albumentations.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                    albumentations.GridDistortion(),\n",
    "                    albumentations.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
    "                    \n",
    "                    ], p=p),\n",
    "                albumentations.ShiftScaleRotate(),\n",
    "                albumentations.CoarseDropout()\n",
    "            ])\n",
    "\n",
    "def get_test_aug(img_size, resize=0.5, crop=0.9):\n",
    "    crop_size = [round(crop*size) for size in img_size]\n",
    "    resize = [round(resize*size) for size in crop_size]\n",
    "    \n",
    "    if crop < 1.0:\n",
    "        crop_size = [crop_size[0], crop_size[0]]\n",
    "        resize = [resize[0], resize[0]]\n",
    "    \n",
    "    return  albumentations.Compose([\n",
    "        albumentations.CenterCrop(height=crop_size[0], width=crop_size[1], always_apply=True),\n",
    "        albumentations.Resize(height=resize[0], width=resize[1], always_apply=True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b315c8-b688-4990-be97-6e82b96507a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_supervision_loss(inp, targ, loss_func=ComboLoss()):\n",
    "    loss = 0\n",
    "    for i in range(len(inp)):\n",
    "        loss += loss_func(inp[i], targ)\n",
    "    return loss/len(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd7c38be-6560-4d94-a7bc-c9671b510e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms_kwargs = dict(train=get_train_aug, test=get_test_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce1ed4d1-96d7-4436-9fc0-cb8dcf94d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:1131: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "dls, dev = get_5C_25D_dls(train, channels=channels, bs=64, crop=0.6, val_crop=1.0, resize=1.0, aug='albumentations',\n",
    "                          sample_empty=False, val='group', fold=fold, tfms_kwargs=tfms_kwargs,\n",
    "                          show=False, aug_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9dfbd71-464e-4289-999d-63aa11c3f361",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_custom_learner(dls,\n",
    "                           UNetPlusPlus('regnety_160', channels, 3, pretrained=True),\n",
    "                           deep_supervision_loss,\n",
    "                           unetplusplus_splitter,\n",
    "                           [dice_coeff_deep, custom_metric_deep],\n",
    "                           cbs=[GradientAccumulation(64), MixUp()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9bbdf624-79c4-444b-919b-a219b88e7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model = learn.model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8dab7694-543c-47e0-b89f-11de41335dde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muw-madison models\u001b[39m\u001b[38;5;124m\"\u001b[39m, group\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-f\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     dls, dev \u001b[38;5;241m=\u001b[39m get_5C_25D_dls(train, channels\u001b[38;5;241m=\u001b[39mchannels, bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, val_crop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, resize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, aug\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malbumentations\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                           sample_empty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, val\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m, fold\u001b[38;5;241m=\u001b[39mfold, tfms_kwargs\u001b[38;5;241m=\u001b[39mtfms_kwargs,\n\u001b[1;32m      6\u001b[0m                           show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, aug_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      8\u001b[0m     learn \u001b[38;5;241m=\u001b[39m get_custom_learner(dls,\n\u001b[1;32m      9\u001b[0m                            UNetPlusPlus(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregnety_160\u001b[39m\u001b[38;5;124m'\u001b[39m, channels, \u001b[38;5;241m3\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     10\u001b[0m                            deep_supervision_loss,\n\u001b[1;32m     11\u001b[0m                            unetplusplus_splitter,\n\u001b[1;32m     12\u001b[0m                            [dice_coeff_deep, custom_metric_deep],\n\u001b[1;32m     13\u001b[0m                            cbs\u001b[38;5;241m=\u001b[39m[GradientAccumulation(\u001b[38;5;241m64\u001b[39m), MixUp()])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    wandb.init(project=\"uw-madison models\", group=f'{run}', name=f'{run}-f{fold}')\n",
    "    \n",
    "    dls, dev = get_5C_25D_dls(train, channels=channels, bs=64, crop=0.6, val_crop=1.0, resize=1.0, aug='albumentations',\n",
    "                          sample_empty=False, val='group', fold=fold, tfms_kwargs=tfms_kwargs,\n",
    "                          show=False, aug_p=0.5)\n",
    "    \n",
    "    learn = get_custom_learner(dls,\n",
    "                           UNetPlusPlus('regnety_160', channels, 3, pretrained=True),\n",
    "                           deep_supervision_loss,\n",
    "                           unetplusplus_splitter,\n",
    "                           [dice_coeff_deep, custom_metric_deep],\n",
    "                           cbs=[GradientAccumulation(64), MixUp()])\n",
    "    \n",
    "    learn.unfreeze()\n",
    "    \n",
    "    learn.fit_one_cycle(100, 6e-3, cbs=[SaveModelCallback('dice_coeff_deep', fname=f'{run}-f{fold}', with_opt=True, save_at_epoch=99),\n",
    "                                        WandbCallback(log_preds=False, model_name=f'{run}-f{fold}')])\n",
    "    \n",
    "    learn.export(f'{run}-f{fold}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae7e42-5369-40dd-9e0b-43db94566e25",
   "metadata": {},
   "source": [
    "## Export only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba80e60-adf1-4d99-a789-3b4863c271b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(5):\n",
    "    dls, dev = get_5C_25D_dls(train, channels=channels, bs=64, crop=0.6, val_crop=1.0, resize=1.0, aug='albumentations',\n",
    "                          sample_empty=False, val='group', fold=fold, tfms_kwargs=tfms_kwargs,\n",
    "                          show=False, aug_p=0.5)\n",
    "    \n",
    "    learn = get_custom_learner(dls,\n",
    "                           UNetPlusPlus('regnety_160', channels, 3, pretrained=True),\n",
    "                           deep_supervision_loss,\n",
    "                           unetplusplus_splitter,\n",
    "                           [dice_coeff_deep, custom_metric_deep],\n",
    "                           cbs=[GradientAccumulation(64), MixUp()])\n",
    "    \n",
    "    learn.unfreeze()\n",
    "    \n",
    "    learn.load(f'{run}-f{fold}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
